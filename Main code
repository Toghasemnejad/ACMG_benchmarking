#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Updated Variant Classification Tools Benchmarking Script

This script benchmarks variant classification tools against manual annotations,
with improvements in ranking, statistical tests, and visualizations.
"""

import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn import metrics
from statsmodels.stats import inter_rater as irr
from statsmodels.stats.contingency_tables import mcnemar, SquareTable
import warnings
from collections import defaultdict
import itertools
from matplotlib_venn import venn2, venn3
from lifelines import KaplanMeierFitter, CoxPHFitter
from lifelines.statistics import logrank_test
import statsmodels.api as sm
from sklearn.utils import resample
from scipy.stats import friedmanchisquare, fisher_exact
from statsmodels.stats.multicomp import pairwise_tukeyhsd
import scikit_posthocs as sp
import matplotlib.patches as patches
from statsmodels.stats.multitest import multipletests

# Set larger font sizes for all text elements
plt.rcParams.update({
    'font.size': 24,              # Base font size
    'font.family': 'serif',
    'font.serif': ['Times New Roman'],
    'axes.labelsize': 28,         # Axis labels
    'axes.titlesize': 32,         # Axis title
    'xtick.labelsize': 24,        # X-tick labels
    'ytick.labelsize': 24,        # Y-tick labels
    'legend.fontsize': 24,        # Legend text
    'figure.titlesize': 32        # Figure title
})
sns.set_context("poster")  # Use the largest preset context

# Suppress warnings
warnings.filterwarnings('ignore')

class VariantToolBenchmarker:
    """Class to benchmark variant classification tools."""
    
    def __init__(self, acmg_path, lirical_folder, manual_path, exclude_tools=None, sample_subset=None):
        """
        Initialize the benchmarker with paths to data files.
        
        Parameters:
        -----------
        acmg_path : str
            Path to unified ACMG data CSV file
        lirical_folder : str
            Path to folder containing LIRICAL files
        manual_path : str
            Path to manual annotations Excel file
        exclude_tools : list, optional
            List of tools to exclude from analysis (default: ['charger', 'cpsr'])
        sample_subset : list, optional
            List of sample IDs to include in the analysis (default: all samples)
        """
        self.acmg_path = acmg_path
        self.lirical_folder = lirical_folder
        self.manual_path = manual_path
        self.exclude_tools = exclude_tools if exclude_tools is not None else ['charger', 'cpsr']
        self.sample_subset = sample_subset
        
        # Initialize result containers
        self.results = {}
        self.tool_data = {}
        self.metrics = {}
        self.statistical_tests = {}  # To store statistical test results

    def load_data(self):
        """Load and preprocess all data sources."""
        print("Loading and preprocessing data...")
        
        # Load unified ACMG data
        self.acmg_data = pd.read_csv(self.acmg_path)
        print(f"Loaded ACMG data: {self.acmg_data.shape[0]} rows")
        
        # Filter out excluded tools
        self.acmg_filtered = self.acmg_data[~self.acmg_data['tool'].isin(self.exclude_tools)]
        print(f"After filtering {', '.join(self.exclude_tools)}: {self.acmg_filtered.shape[0]} rows")
        
        # Load manual annotations (ground truth)
        self.manual_data = pd.read_excel(self.manual_path)
        print(f"Loaded manual annotations: {self.manual_data.shape[0]} rows")
        # Fix column names if needed
        if 'sample_id' in self.manual_data.columns and 'Sample id' not in self.manual_data.columns:
            self.manual_data = self.manual_data.rename(columns={'sample_id': 'Sample id'})
            print("Renamed 'sample_id' column to 'Sample id'")
        # Filter by sample subset if provided
        if self.sample_subset:
            self.manual_data = self.manual_data[self.manual_data['Sample id'].isin(self.sample_subset)]
            self.acmg_filtered = self.acmg_filtered[self.acmg_filtered['sample_id'].isin(self.sample_subset)]
            print(f"Filtered to {len(self.sample_subset)} samples: {self.manual_data.shape[0]} manual annotations, {self.acmg_filtered.shape[0]} ACMG variants")
        
        # Load LIRICAL data with proper encoding
        self.lirical_data = self._load_lirical_data()
        if self.lirical_data is not None:
            print(f"Loaded LIRICAL data: {self.lirical_data.shape[0]} rows")
            # Filter by sample subset if provided
            if self.sample_subset:
                self.lirical_data = self.lirical_data[self.lirical_data['sample_id'].isin(self.sample_subset)]
                print(f"Filtered LIRICAL data to {len(self.sample_subset)} samples: {self.lirical_data.shape[0]} rows")
        
        # Analyze Franklin data to understand its characteristics
        self._check_franklin_data()
        
        # Create a unified dataset by tool
        self._prepare_tool_data()
        print("Data loading and preprocessing complete.")

    def _check_franklin_data(self):
        """
        Special function to check Franklin data and report its characteristics.
        This helps understand how Franklin prioritizes variants and what columns might be useful.
        """
        # Get Franklin data
        franklin_df = self.acmg_filtered[self.acmg_filtered['tool'] == 'franklin'].copy()
        
        if franklin_df.empty:
            print("No Franklin data found")
            return
        
        print(f"\nAnalyzing Franklin data: {len(franklin_df)} variants across {franklin_df['sample_id'].nunique()} samples")
        
        # Check classifications
        class_counts = franklin_df['classification'].value_counts()
        total_variants = len(franklin_df)
        
        print("\nClassification distribution:")
        for cls, count in class_counts.items():
            percentage = (count / total_variants) * 100
            print(f"  {cls}: {count} ({percentage:.1f}%)")
        
        # Check available columns that might be useful for ranking
        print("\nPotentially useful columns for ranking:")
        important_keywords = ['rank', 'score', 'priority', 'impact', 'effect', 'consequence', 
                             'cadd', 'freq', 'maf', 'vaf', 'pathogenic']
        
        useful_columns = []
        for col in franklin_df.columns:
            if any(keyword in col.lower() for keyword in important_keywords):
                useful_columns.append(col)
        
        for col in useful_columns:
            # Get some statistics for this column
            try:
                if franklin_df[col].dtype in ['int64', 'float64']:
                    print(f"  {col}: numeric, range={franklin_df[col].min()}-{franklin_df[col].max()}, unique values={franklin_df[col].nunique()}")
                else:
                    print(f"  {col}: {franklin_df[col].dtype}, unique values={franklin_df[col].nunique()}")
            except:
                print(f"  {col}: unknown type")
        
        # Check variant counts per sample
        variant_counts = franklin_df.groupby('sample_id').size()
        print(f"\nVariant counts per sample: min={variant_counts.min()}, max={variant_counts.max()}, mean={variant_counts.mean():.1f}")
        
        # Check if all samples have the same number of variants
        if variant_counts.nunique() == 1:
            print(f"All samples have exactly {variant_counts.iloc[0]} variants - suggests data was pre-filtered")
        
        # Return useful columns for use in ranking
        return useful_columns

    def _load_lirical_data(self):
        """
        Load LIRICAL data from a single combined TSV file instead of multiple files.
        """
        lirical_file = os.path.join(self.lirical_folder, "LIRICAL_163.tsv")
        
        if not os.path.exists(lirical_file):
            print(f"LIRICAL file not found: {lirical_file}")
            return None
        
        print(f"Loading combined LIRICAL file: {lirical_file}")
        
        try:
            # Load TSV file
            lirical_data = pd.read_csv(lirical_file, sep='\t')
            
            # Verify required columns exist
            required_columns = ['rank', 'sample_id', 'hgnc_gene']
            missing_columns = [col for col in required_columns if col not in lirical_data.columns]
            
            if missing_columns:
                print(f"Error: Missing required columns in LIRICAL data: {', '.join(missing_columns)}")
                return None
            
            # Ensure LIRICAL tool name is consistent
            if 'tool' in lirical_data.columns:
                lirical_data['tool'] = lirical_data['tool'].str.lower()
            else:
                lirical_data['tool'] = 'lirical'
                
            print(f"Loaded LIRICAL data with {len(lirical_data)} rows across {lirical_data['sample_id'].nunique()} samples")
            return lirical_data
            
        except Exception as e:
            print(f"Error loading LIRICAL data: {e}")
            return None

    def _prepare_tool_data(self):
        """
        Updated function to prepare data for each tool using native ranking systems.
        This treats all tools similar to LIRICAL, preserving their native prioritization.
        """
        # Get list of tools (excluding specified tools)
        tools = self.acmg_filtered['tool'].unique().tolist()
        
        # Add LIRICAL if available
        if self.lirical_data is not None:
            tools.append('LIRICAL')
        
        print(f"Preparing data for tools: {', '.join(tools)}")
        
        # Get ground truth genes for reference
        ground_truth = {}
        for _, row in self.manual_data.iterrows():
            sample_id = row['Sample id']
            gene = row['hgnc_gene']
            ground_truth[sample_id] = gene
        
        # For each tool, create a dataset with ranking information
        for tool in tools:
            if tool != 'LIRICAL':
                # Get data for this tool
                tool_df = self.acmg_filtered[self.acmg_filtered['tool'] == tool].copy()
                
                # Check if we have data for this tool
                if tool_df.empty:
                    print(f"Warning: No data found for tool {tool}")
                    continue
                
                # Debug info
                print(f"Processing {tool} with {len(tool_df)} variants across {tool_df['sample_id'].nunique()} samples")
                
                # Create a mapping for each sample's gene rankings
                sample_gene_rankings = {}
                
                # Process each sample separately
                for sample_id, group in tool_df.groupby('sample_id'):
                    # Skip if sample isn't in ground truth
                    if sample_id not in ground_truth:
                        continue
                    
                    try:
                        # For all tools, use their native ranking (similar to LIRICAL)
                        # Check for tool-specific ranking columns
                        if tool == 'franklin':
                            # Check if there's a ranking column specific to Franklin
                            franklin_rank_cols = [col for col in group.columns if any(x in col.lower() for x in ['rank', 'score', 'priority']) and 'franklin' in col.lower()]
                            
                            if franklin_rank_cols:
                                # Use Franklin's own ranking column
                                rank_col = franklin_rank_cols[0]
                                print(f"Using {rank_col} for Franklin ranking")
                                sorted_df = group.sort_values(by=[rank_col], ascending=True if 'rank' in rank_col.lower() else False)
                            else:
                                # Preserve original order - Franklin likely already sorted variants by importance
                                # This respects Franklin's native prioritization
                                sorted_df = group
                        
                        elif tool == 'genebe':
                            # Check for GeneBE ranking columns
                            genebe_rank_cols = [col for col in group.columns if any(x in col.lower() for x in ['rank', 'score', 'priority']) and 'genebe' in col.lower()]
                            
                            if genebe_rank_cols:
                                rank_col = genebe_rank_cols[0]
                                print(f"Using {rank_col} for GeneBE ranking")
                                sorted_df = group.sort_values(by=[rank_col], ascending=True if 'rank' in rank_col.lower() else False)
                            else:
                                # If no specific ranking column, use order as is
                                sorted_df = group
                        
                        elif tool == 'intervar':
                            # Check for InterVar ranking columns
                            intervar_rank_cols = [col for col in group.columns if any(x in col.lower() for x in ['rank', 'score', 'priority']) and 'intervar' in col.lower()]
                            
                            if intervar_rank_cols:
                                rank_col = intervar_rank_cols[0]
                                print(f"Using {rank_col} for InterVar ranking")
                                sorted_df = group.sort_values(by=[rank_col], ascending=True if 'rank' in rank_col.lower() else False)
                            else:
                                # If no specific ranking column, use order as is
                                sorted_df = group
                        
                        elif tool == 'tapes':
                            # Check for TAPES ranking columns
                            tapes_rank_cols = [col for col in group.columns if any(x in col.lower() for x in ['rank', 'score', 'priority']) and 'tapes' in col.lower()]
                            
                            if tapes_rank_cols:
                                rank_col = tapes_rank_cols[0]
                                print(f"Using {rank_col} for TAPES ranking")
                                sorted_df = group.sort_values(by=[rank_col], ascending=True if 'rank' in rank_col.lower() else False)
                            else:
                                # If no specific ranking column, use order as is
                                sorted_df = group
                        
                        elif tool == 'cpsr':
                            # Check for CPSR ranking columns
                            cpsr_rank_cols = [col for col in group.columns if any(x in col.lower() for x in ['rank', 'score', 'priority']) and 'cpsr' in col.lower()]
                            
                            if cpsr_rank_cols:
                                rank_col = cpsr_rank_cols[0]
                                print(f"Using {rank_col} for CPSR ranking")
                                sorted_df = group.sort_values(by=[rank_col], ascending=True if 'rank' in rank_col.lower() else False)
                            else:
                                # If no specific ranking column, use order as is
                                sorted_df = group
                        
                        else:
                            # For any other tool, check for ranking columns
                            rank_cols = [col for col in group.columns if any(x in col.lower() for x in ['rank', 'score', 'priority'])]
                            
                            if rank_cols:
                                rank_col = rank_cols[0]
                                print(f"Using {rank_col} for {tool} ranking")
                                sorted_df = group.sort_values(by=[rank_col], ascending=True if 'rank' in rank_col.lower() else False)
                            else:
                                # If no ranking column found, preserve original order
                                sorted_df = group
                        
                        # Extract sorted genes
                        sorted_genes = []
                        seen = set()
                        for gene in sorted_df['hgnc_gene'].tolist():
                            if gene not in seen:
                                sorted_genes.append(gene)
                                seen.add(gene)
                        
                        # Store rankings
                        sample_gene_rankings[sample_id] = sorted_genes
                        
                        # Debug info for top genes
                        true_gene = ground_truth[sample_id]
                        if true_gene in sorted_genes:
                            rank = sorted_genes.index(true_gene) + 1
                            if rank <= 5:  # Only log for top-ranked matches
                                print(f"  {tool} - {sample_id}: Found {true_gene} at rank {rank}/{len(sorted_genes)}")
                    
                    except Exception as e:
                        print(f"Error processing {tool} data for sample {sample_id}: {e}")
                        # Fallback: just use genes as they appear
                        sorted_genes = group['hgnc_gene'].unique().tolist()
                        sample_gene_rankings[sample_id] = sorted_genes
                
                # Store all sample rankings for this tool
                self.tool_data[tool] = sample_gene_rankings
                
                # Calculate and display summary metrics
                self._calculate_tool_summary(tool, sample_gene_rankings, ground_truth)
            
            else:  # Special handling for LIRICAL
                # Process LIRICAL data if available
                if self.lirical_data is not None:
                    print(f"Processing LIRICAL data with {len(self.lirical_data)} rows across {self.lirical_data['sample_id'].nunique()} samples")
                    
                    # No need to check for rank column as it's already in the unified data
                    sample_gene_rankings = {}
                    
                    for sample_id, group in self.lirical_data.groupby('sample_id'):
                        try:
                            # Sort by rank (ascending = lowest rank first)
                            sorted_genes = group.sort_values(by=['rank'], ascending=True)['hgnc_gene'].tolist()
                            
                            # Remove duplicates
                            seen = set()
                            sorted_genes = [g for g in sorted_genes if not (g in seen or seen.add(g))]
                            
                            sample_gene_rankings[sample_id] = sorted_genes
                            
                            # Check if ground truth gene is found
                            if sample_id in ground_truth:
                                true_gene = ground_truth[sample_id]
                                if true_gene in sorted_genes:
                                    rank = sorted_genes.index(true_gene) + 1
                                    if rank <= 5:  # Only log for top-ranked matches
                                        print(f"  LIRICAL - {sample_id}: Found {true_gene} at rank {rank}/{len(sorted_genes)}")
                                        
                        except Exception as e:
                            print(f"Error processing LIRICAL data for sample {sample_id}: {e}")
                    
                    # Store LIRICAL rankings
                    self.tool_data[tool] = sample_gene_rankings
                    
                    # Calculate and display summary metrics
                    self._calculate_tool_summary(tool, sample_gene_rankings, ground_truth)

    def _calculate_tool_summary(self, tool, sample_gene_rankings, ground_truth):
        """Calculate and display summary metrics for a tool."""
        found_count = 0
        top1_count = 0
        top5_count = 0
        top10_count = 0
        for sample_id, true_gene in ground_truth.items():
            if sample_id in sample_gene_rankings:
                if true_gene in sample_gene_rankings[sample_id]:
                    found_count += 1
                    rank = sample_gene_rankings[sample_id].index(true_gene) + 1
                    if rank == 1:
                        top1_count += 1
                    if rank <= 5:
                        top5_count += 1
                    if rank <= 10:
                        top10_count += 1
        
        total_samples = len([s for s in ground_truth.keys() if s in sample_gene_rankings])
        print(f"Tool {tool} summary:")
        print(f"  Found {found_count} out of {total_samples} ground truth genes overall")
        if total_samples > 0:
            print(f"  Top-1 accuracy: {top1_count/total_samples*100:.1f}%")
            print(f"  Top-5 accuracy: {top5_count/total_samples*100:.1f}%")
            print(f"  Top-10 accuracy: {top10_count/total_samples*100:.1f}%")
            print(f"  Not found: {total_samples - found_count} ({(total_samples - found_count)/total_samples*100:.1f}%)")

    def calculate_metrics(self):
        """Calculate all performance metrics for each tool."""
        print("Calculating performance metrics...")
        
        for tool, rankings in self.tool_data.items():
            print(f"Processing tool: {tool}")
            
            tool_metrics = {
                'ranking': {},
                'filtering': {},
                'accuracy': {}
            }
            
            # Calculate ranking metrics
            tool_metrics['ranking'] = self._calculate_ranking_metrics(rankings)
            
            # Calculate filtering metrics
            tool_metrics['filtering'] = self._calculate_filtering_metrics(rankings)
            
            # Calculate accuracy metrics
            tool_metrics['accuracy'] = self._calculate_accuracy_metrics(rankings)
            
            # Store metrics for this tool
            self.metrics[tool] = tool_metrics
        
        print("Performance metrics calculation complete.")

    def _calculate_ranking_metrics(self, rankings):
        """
        Calculate ranking metrics:
        - Top-rank percentage
        - Top-N percentage (N=5,10,20,30,40,50)
        - Mean and median rank (only for found cases)
        - Min/max rank (only for found cases)
        """
        metrics = {}
        
        ground_truth = {row['Sample id']: row['hgnc_gene'] for _, row in self.manual_data.iterrows()}
        
        found_ranks = []  # Only collect ranks where the gene is found
        top_1_count = 0
        top_n_counts = {5: 0, 10: 0, 20: 0, 30: 0, 40: 0, 50: 0}
        total_samples = 0
        
        for sample_id, true_gene in ground_truth.items():
            if sample_id in rankings:
                total_samples += 1
                ranked_genes = rankings[sample_id]
                
                if true_gene in ranked_genes:
                    rank = ranked_genes.index(true_gene) + 1
                    found_ranks.append(rank)  # Collect rank only if found
                    if rank == 1:
                        top_1_count += 1
                    for n in top_n_counts.keys():
                        if rank <= n:
                            top_n_counts[n] += 1
        
        if total_samples > 0:
            metrics['top_rank_percentage'] = (top_1_count / total_samples) * 100
            for n, count in top_n_counts.items():
                metrics[f'top_{n}_percentage'] = (count / total_samples) * 100
        else:
            metrics['top_rank_percentage'] = np.nan
            for n in top_n_counts.keys():
                metrics[f'top_{n}_percentage'] = np.nan
        
        if found_ranks:
            metrics['mean_rank'] = np.mean(found_ranks)
            metrics['median_rank'] = np.median(found_ranks)
            metrics['min_rank'] = np.min(found_ranks)
            metrics['max_rank'] = np.max(found_ranks)
        else:
            metrics['mean_rank'] = np.nan
            metrics['median_rank'] = np.nan
            metrics['min_rank'] = np.nan
            metrics['max_rank'] = np.nan
        
        return metrics

    def _calculate_filtering_metrics(self, rankings):
        """
        Calculate filtering metrics:
        - Filtered-out rate
        - Retention rate
        """
        metrics = {}
        
        ground_truth = {row['Sample id']: row['hgnc_gene'] for _, row in self.manual_data.iterrows()}
        
        filtered_out_count = 0
        retained_count = 0
        total_samples = 0
        
        for sample_id, true_gene in ground_truth.items():
            if sample_id in rankings:
                total_samples += 1
                ranked_genes = rankings[sample_id]
                if true_gene in ranked_genes:
                    retained_count += 1
                else:
                    filtered_out_count += 1
        
        if total_samples > 0:
            metrics['filtered_out_rate'] = (filtered_out_count / total_samples) * 100
            metrics['retention_rate'] = (retained_count / total_samples) * 100
        else:
            metrics['filtered_out_rate'] = np.nan
            metrics['retention_rate'] = np.nan
        
        return metrics

    def _calculate_accuracy_metrics(self, rankings):
        """
        Calculate accuracy metrics:
        - Precision at different rank thresholds
        - Recall (sensitivity)
        - Specificity
        - F1 score
        """
        metrics = {}
        
        ground_truth = {row['Sample id']: row['hgnc_gene'] for _, row in self.manual_data.iterrows()}
        
        rank_thresholds = [1, 5, 10, 20, 50]
        
        # Calculate precision at different thresholds using global precision
        for threshold in rank_thresholds:
            true_positives = 0
            total_predictions = 0
            
            for sample_id, true_gene in ground_truth.items():
                if sample_id in rankings:
                    ranked_genes = rankings[sample_id]
                    # Take top k genes (or all if less than k)
                    top_k_genes = ranked_genes[:min(threshold, len(ranked_genes))]
                    
                    total_predictions += len(top_k_genes)
                    if true_gene in top_k_genes:
                        true_positives += 1
            
            if total_predictions > 0:
                precision = true_positives / total_predictions
            else:
                precision = 0.0
                
            metrics[f'precision_at_{threshold}'] = precision
        
        true_positives = 0
        false_negatives = 0
        for sample_id, true_gene in ground_truth.items():
            if sample_id in rankings:
                ranked_genes = rankings[sample_id]
                if true_gene in ranked_genes:
                    true_positives += 1
                else:
                    false_negatives += 1
        if true_positives + false_negatives > 0:
            recall = true_positives / (true_positives + false_negatives)
        else:
            recall = np.nan
        metrics['recall'] = recall
        
        true_negatives = 0
        false_positives = 0
        for sample_id, true_gene in ground_truth.items():
            if sample_id in rankings:
                ranked_genes = rankings[sample_id]
                true_negatives += len(set(self.acmg_filtered['hgnc_gene'].unique()) - set(ranked_genes))
                false_positives += len(ranked_genes) - (1 if true_gene in ranked_genes else 0)
        if true_negatives + false_positives > 0:
            specificity = true_negatives / (true_negatives + false_positives)
        else:
            specificity = np.nan
        metrics['specificity'] = specificity
        
        if recall + metrics['precision_at_1'] > 0:
            f1 = 2 * (metrics['precision_at_1'] * recall) / (metrics['precision_at_1'] + recall)
        else:
            f1 = np.nan
        metrics['f1_score'] = f1
        
        return metrics

    def perform_and_visualize_statistical_tests(self, output_dir='viz'):
        """
        Perform statistical tests and generate visualizations with fixes:
        - Bonferroni correction for Fisher's exact tests
        - Sample size checks
        - Bootstrap CIs for top-1, top-5, top-10
        - Fisher’s exact test visualization
        - Better missing data handling
        """
        print("Performing statistical tests...")
        
        os.makedirs(output_dir, exist_ok=True)
        self.statistical_tests = {
            'bootstrap_ci': {},
            'friedman_test': {},
            'nemenyi_test': {},
            'fisher_exact': {}
        }
        
        tools = list(self.tool_data.keys())
        ground_truth = {row['Sample id']: row['hgnc_gene'] for _, row in self.manual_data.iterrows()}
        sample_ids = sorted(list(set(ground_truth.keys()) & set().union(*[set(self.tool_data[t].keys()) for t in tools])))
        n_samples = len(sample_ids)
        print(f"Samples for statistical tests: {n_samples}")
        
        if n_samples < 10:
            print("Warning: Small sample size (<10) may affect test reliability")
        
        # Bootstrap Confidence Intervals
        print("Calculating bootstrap CIs for Top-1, Top-5, Top-10...")
        n_resamples = 1000
        metrics_list = ['top_rank_percentage', 'top_5_percentage', 'top_10_percentage']
        labels = ['Top-1 Accuracy', 'Top-5 Accuracy', 'Top-10 Accuracy']
        
        for tool in tools:
            self.statistical_tests['bootstrap_ci'][tool] = {}
            for metric, label in zip(metrics_list, labels):
                samples = []
                threshold = 1 if metric == 'top_rank_percentage' else int(metric.split('_')[1])
                for sample_id, true_gene in ground_truth.items():
                    if sample_id in self.tool_data[tool]:
                        ranked_genes = self.tool_data[tool][sample_id]
                        found = true_gene in ranked_genes and ranked_genes.index(true_gene) < threshold
                        samples.append(1 if found else 0)
                
                if len(samples) < 5:
                    print(f"Warning: Too few samples ({len(samples)}) for {tool} {label} CI")
                    self.statistical_tests['bootstrap_ci'][tool][metric] = {'lower': np.nan, 'upper': np.nan, 'mean': np.nan}
                    continue
                
                bootstrap_results = [np.mean(resample(samples)) * 100 for _ in range(n_resamples)]
                lower, upper = np.percentile(bootstrap_results, [2.5, 97.5])
                self.statistical_tests['bootstrap_ci'][tool][metric] = {
                    'lower': lower,
                    'upper': upper,
                    'mean': np.mean(bootstrap_results)
                }
        
        plt.figure(figsize=(15, 5))
        for i, (metric, label) in enumerate(zip(metrics_list, labels)):
            plt.subplot(1, 3, i+1)
            means, lowers, uppers, tool_names = [], [], [], []
            for tool in tools:
                if metric in self.statistical_tests['bootstrap_ci'][tool]:
                    ci_data = self.statistical_tests['bootstrap_ci'][tool][metric]
                    if not np.isnan(ci_data['mean']):
                        tool_names.append(tool)
                        means.append(ci_data['mean'])
                        lowers.append(ci_data['lower'])
                        uppers.append(ci_data['upper'])
            
            yerr = [np.array(means) - np.array(lowers), np.array(uppers) - np.array(means)]
            plt.errorbar(range(len(tool_names)), means, yerr=yerr, fmt='o', capsize=5)
            plt.title(f"{label} (95% CI)")
            plt.ylabel("Percentage (%)")
            plt.xticks(range(len(tool_names)), tool_names, rotation=45)
            plt.grid(True, linestyle='--', alpha=0.7)
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'bootstrap_ci.png'), dpi=1200, bbox_inches='tight')
        plt.close()
        
        # Friedman & Nemenyi Tests
        print("Performing Friedman and Nemenyi tests...")
        if n_samples >= 5 and len(tools) >= 2:
            rank_matrix = []
            for sample_id in sample_ids:
                sample_ranks = []
                for tool in tools:
                    if sample_id in self.tool_data[tool]:
                        ranked_genes = self.tool_data[tool][sample_id]
                        rank = ranked_genes.index(ground_truth[sample_id]) + 1 if ground_truth[sample_id] in ranked_genes else len(ranked_genes) + 1
                    else:
                        # Impute average rank for missing data
                        avg_rank = np.mean([len(self.tool_data[tool].get(sid, [])) + 1 
                                          for sid in self.tool_data[tool].keys() 
                                          if ground_truth.get(sid) not in self.tool_data[tool].get(sid, [])])
                        rank = avg_rank if not np.isnan(avg_rank) else 1000
                    sample_ranks.append(rank)
                rank_matrix.append(sample_ranks)
            
            rank_matrix = np.array(rank_matrix)
            try:
                statistic, p_value = friedmanchisquare(*rank_matrix.T)
                self.statistical_tests['friedman_test'] = {
                    'statistic': statistic,
                    'p_value': p_value,
                    'significant': p_value < 0.05
                }
                print(f"Friedman test: statistic={statistic:.2f}, p-value={p_value:.4f}")
                
                if p_value < 0.05:
                    nemenyi_result = sp.posthoc_nemenyi_friedman(rank_matrix)
                    self.statistical_tests['nemenyi_test'] = {'matrix': nemenyi_result, 'tool_pairs': {}}
                    for i, tool1 in enumerate(tools):
                        for j, tool2 in enumerate(tools):
                            if i < j:
                                p_val = nemenyi_result.iloc[i, j]
                                self.statistical_tests['nemenyi_test']['tool_pairs'][(tool1, tool2)] = {
                                    'p_value': p_val,
                                    'significant': p_val < 0.05
                                }
                                if p_val < 0.05:
                                    print(f"Significant difference: {tool1} vs {tool2}, p={p_val:.4f}")
            
                    plt.figure(figsize=(10, 8))
                    p_matrix = np.ones((len(tools), len(tools)))
                    for (tool1, tool2), data in self.statistical_tests['nemenyi_test']['tool_pairs'].items():
                        i, j = tools.index(tool1), tools.index(tool2)
                        p_matrix[i, j] = p_matrix[j, i] = data['p_value']
                    
                    mask = np.triu(np.ones_like(p_matrix, dtype=bool))
                    sns.heatmap(p_matrix, mask=mask, cmap='YlOrRd_r', vmin=0, vmax=0.1,
                               annot=True, fmt='.3f', cbar_kws={'label': 'p-value'})
                    plt.title("Nemenyi Test: Pairwise p-values")
                    plt.xticks(np.arange(len(tools)) + 0.5, tools, rotation=45)
                    plt.yticks(np.arange(len(tools)) + 0.5, tools)
                    plt.tight_layout()
                    plt.savefig(os.path.join(output_dir, 'nemenyi_test.png'), dpi=1200)
                    plt.close()
            except Exception as e:
                print(f"Friedman test error: {e}")
        else:
            print("Need >=5 samples and >=2 tools for Friedman test")
        
        # Fisher’s Exact Test
        print("Performing Fisher’s exact test with Bonferroni correction...")
        self.statistical_tests['fisher_exact'] = {'top_1': {}, 'top_5': {}, 'top_10': {}}
        thresholds = [1, 5, 10]
        keys = ['top_1', 'top_5', 'top_10']
        all_p_values = []
        all_pairs = []
        
        for threshold, key in zip(thresholds, keys):
            for i, tool1 in enumerate(tools):
                for j, tool2 in enumerate(tools):
                    if i < j:
                        table = np.zeros((2, 2), dtype=int)
                        count = 0
                        for sample_id, true_gene in ground_truth.items():
                            if sample_id in self.tool_data[tool1] and sample_id in self.tool_data[tool2]:
                                found1 = true_gene in self.tool_data[tool1][sample_id] and self.tool_data[tool1][sample_id].index(true_gene) < threshold
                                found2 = true_gene in self.tool_data[tool2][sample_id] and self.tool_data[tool2][sample_id].index(true_gene) < threshold
                                table[int(found1), int(found2)] += 1
                                count += 1
                        
                        if count < 10:
                            print(f"Warning: Low sample count ({count}) for {tool1} vs {tool2} at top-{threshold}")
                        if np.sum(table) == 0:
                            continue
                        
                        try:
                            odds_ratio, p_value = fisher_exact(table)
                            all_p_values.append(p_value)
                            all_pairs.append((tool1, tool2, threshold, key, table, odds_ratio))
                        except Exception as e:
                            print(f"Fisher’s test error for {tool1} vs {tool2}: {e}")
        
        if all_p_values:
            corrected_p = multipletests(all_p_values, method='bonferroni')[1]
            for (tool1, tool2, threshold, key, table, odds_ratio), p in zip(all_pairs, corrected_p):
                tool1_acc = self.metrics[tool1]['ranking'].get('top_rank_percentage' if threshold == 1 else f'top_{threshold}_percentage', 0)
                tool2_acc = self.metrics[tool2]['ranking'].get('top_rank_percentage' if threshold == 1 else f'top_{threshold}_percentage', 0)
                better = tool1 if tool1_acc > tool2_acc else tool2
                self.statistical_tests['fisher_exact'][key][(tool1, tool2)] = {
                    'odds_ratio': odds_ratio,
                    'p_value': p,
                    'significant': p < 0.05,
                    'contingency_table': table.tolist(),
                    'better_tool': better
                }
                if p < 0.05:
                    print(f"Significant: top-{threshold}, {tool1} vs {tool2}, corrected p={p:.4f}, {better} better")
        
        for threshold, key in zip(thresholds, keys):
            plt.figure(figsize=(8, 6))
            p_matrix = np.ones((len(tools), len(tools)))
            for (tool1, tool2), data in self.statistical_tests['fisher_exact'][key].items():
                i, j = tools.index(tool1), tools.index(tool2)
                p_matrix[i, j] = p_matrix[j, i] = data['p_value']
            
            mask = np.triu(np.ones_like(p_matrix, dtype=bool))
            sns.heatmap(p_matrix, mask=mask, cmap='YlOrRd_r', vmin=0, vmax=0.1,
                       annot=True, fmt='.3f', cbar_kws={'label': 'Corrected p-value'})
            plt.title(f"Fisher’s Exact Test: Top-{threshold}")
            plt.xticks(np.arange(len(tools)) + 0.5, tools, rotation=45)
            plt.yticks(np.arange(len(tools)) + 0.5, tools)
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, f'fisher_exact_top_{threshold}.png'), dpi=1200)
            plt.close()

    def plot_friedman_ranks(self, output_dir='viz'):
        """
        Plot Friedman test ranks with accurate critical distance
        """
        os.makedirs(output_dir, exist_ok=True)
        tools = list(self.tool_data.keys())
        ground_truth = {row['Sample id']: row['hgnc_gene'] for _, row in self.manual_data.iterrows()}
        sample_ids = sorted(list(set(ground_truth.keys()) & set().union(*[set(self.tool_data[t].keys()) for t in tools])))
        
        rank_matrix = []
        for sample_id in sample_ids:
            sample_ranks = []
            for tool in tools:
                if sample_id in self.tool_data[tool]:
                    ranked_genes = self.tool_data[tool][sample_id]
                    rank = ranked_genes.index(ground_truth[sample_id]) + 1 if ground_truth[sample_id] in ranked_genes else len(ranked_genes) + 1
                else:
                    avg_rank = np.mean([len(self.tool_data[tool].get(sid, [])) + 1 
                                      for sid in self.tool_data[tool].keys() 
                                      if ground_truth.get(sid) not in self.tool_data[tool].get(sid, [])])
                    rank = avg_rank if not np.isnan(avg_rank) else 1000
                sample_ranks.append(rank)
            rank_matrix.append(sample_ranks)
        
        rank_matrix = np.array(rank_matrix)
        avg_ranks = np.mean(rank_matrix, axis=0)
        
        k, N = len(tools), len(rank_matrix)
        q_alpha = {2: 1.960, 3: 2.343, 4: 2.569, 5: 2.728, 6: 2.850}.get(k, 2.850)
        critical_distance = q_alpha * np.sqrt(k * (k + 1) / (12 * N))
        
        plt.figure(figsize=(10, 6))
        sorted_ranks = sorted(zip(tools, avg_ranks), key=lambda x: x[1])
        tools, ranks = zip(*sorted_ranks)
        plt.barh(tools, ranks, color='skyblue')
        plt.axvline(min(ranks) + critical_distance, color='red', linestyle='--')
        plt.text(0.7, 0.2, f'CD={critical_distance:.2f}', color='red', transform=plt.gcf().transFigure)
        plt.xlabel('Average Rank')
        plt.title('Friedman Test: Average Ranks')
        plt.grid(axis='x', linestyle='--', alpha=0.7)
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'friedman_ranks.png'), dpi=1200)
        plt.close()
        print(f"Friedman ranks plot saved to {output_dir}/friedman_ranks.png")

    def generate_visualizations(self, output_dir='viz'):
        """
        Generate visualizations for the results.
        
        Parameters:
        -----------
        output_dir : str
            Directory to save visualizations
        """
        print("Generating visualizations...")
        
        os.makedirs(output_dir, exist_ok=True)
        
        tools = list(self.tool_data.keys())
        
        ground_truth = {row['Sample id']: row['hgnc_gene'] for _, row in self.manual_data.iterrows()}
        
        # 1. Bar chart: Number of correct matches per tool
        plt.figure(figsize=(14, 8))
        correct_matches = []
        for tool in tools:
            matches = sum(1 for sample_id, true_gene in ground_truth.items() if sample_id in self.tool_data[tool] and true_gene in self.tool_data[tool][sample_id])
            correct_matches.append(matches)
        plt.bar(tools, correct_matches, color='#4CAF50')
        plt.title('Retention Rate per Tool (%)', fontsize=16)
        plt.xlabel('Tool', fontsize=14)
        plt.ylabel('Number of Correct Matches', fontsize=14)
        plt.grid(True, linestyle='--', alpha=0.7, axis='y')
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'gene_match_accuracy.png'), dpi=1200, bbox_inches='tight')
        plt.close()
        
        # 2. Grouped bar chart: Ranking accuracy comparison
        plt.figure(figsize=(16, 10))
        thresholds = [1, 5, 10, 20, 50]
        colors = ['#66c2a5', '#fc8d62', '#8da0cb', '#e78ac3', '#a6d854']
        accuracy_data = np.zeros((len(tools), len(thresholds)))
        for i, tool in enumerate(tools):
            for j, threshold in enumerate(thresholds):
                metric_name = f'top_rank_percentage' if threshold == 1 else f'top_{threshold}_percentage'
                accuracy_data[i, j] = self.metrics[tool]['ranking'].get(metric_name, 0)
        width = 0.15
        x = np.arange(len(tools))
        for i in range(len(thresholds)):
            plt.bar(x + (i - 2) * width, accuracy_data[:, i], width, label=f'Top-{thresholds[i]} Accuracy (%)', color=colors[i])
        plt.title('Ranking Accuracy Comparison Across Tools', fontsize=20)
        plt.xlabel('Tool', fontsize=14)
        plt.ylabel('Accuracy (%)', fontsize=16)
        plt.xticks(x, tools)
        plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.25), ncol=5, fontsize=16)
        plt.subplots_adjust(top=0.75)
        plt.grid(True, linestyle='--', alpha=0.3)
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'ranking_accuracy_comparison.png'), dpi=1200, bbox_inches='tight')
        plt.close()
        
        # 3. Stacked bar: Distribution of variant ranks
        plt.figure(figsize=(14, 8))
        rank_distributions = []
        categories = ['1st', '2nd-5th', '6th-10th', '>10th', 'FONP']
        colors = ['#4daf4a', '#ff7f00', '#377eb8', '#984ea3', '#e41a1c']
        for tool in tools:
            counts = {'1st': 0, '2nd-5th': 0, '6th-10th': 0, '>10th': 0, 'FONP': 0}
            for sample_id, true_gene in ground_truth.items():
                if sample_id in self.tool_data[tool]:
                    ranked_genes = self.tool_data[tool][sample_id]
                    if true_gene in ranked_genes:
                        rank = ranked_genes.index(true_gene) + 1
                        if rank == 1:
                            counts['1st'] += 1
                        elif 2 <= rank <= 5:
                            counts['2nd-5th'] += 1
                        elif 6 <= rank <= 10:
                            counts['6th-10th'] += 1
                        else:
                            counts['>10th'] += 1
                    else:
                        counts['FONP'] += 1
                else:
                    counts['FONP'] += 1
            total = sum(counts.values())
            percentages = {k: (v / total * 100) for k, v in counts.items()}
            rank_distributions.append([percentages[cat] for cat in categories])
        data = np.array(rank_distributions).T
        bottoms = np.zeros(len(tools))
        for i, (cat, color) in enumerate(zip(categories, colors)):
            plt.bar(tools, data[i], bottom=bottoms, label=cat, color=color)
            for j, tool in enumerate(tools):
                if data[i][j] > 5:
                    plt.text(j, bottoms[j] + data[i][j]/2, f"{data[i][j]:.1f}%", ha='center', va='center', color='black', fontsize=16)
            bottoms += data[i]
        plt.title('Overall Distribution of Variant Ranks Across Prioritization Tools', fontsize=16)
        plt.xlabel('Variant prioritization software', fontsize=14)
        plt.ylabel('Disease-causing variants, %', fontsize=14)
        plt.legend(title='Rank', loc='center left', bbox_to_anchor=(1.05, 0.5))
        plt.ylim(0, 100)
        plt.grid(True, linestyle='--', alpha=0.3, axis='y')
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'rank_distribution.png'), dpi=1200, bbox_inches='tight')
        plt.close()
        
        # 4. Radar chart: Overall tool performance metrics
        plt.figure(figsize=(12, 10))
        metrics_radar = ['Top-1 Accuracy (%)', 'Top-5 Accuracy (%)', 'Top-10 Accuracy (%)', 'Accuracy (%)']
        N = len(metrics_radar)
        angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()
        angles += angles[:1]
        ax = plt.subplot(111, polar=True)
        ax.set_theta_offset(np.pi / 2)
        ax.set_theta_direction(-1)
        plt.xticks(angles[:-1], metrics_radar)
        ax.set_rlabel_position(0)
        plt.yticks([20, 40, 60, 80, 100], ["20%", "40%", "60%", "80%", "100%"], color="grey", size=8)
        plt.ylim(0, 100)
        tool_colors = {
            'franklin': '#1f77b4',
            'Franklin': '#1f77b4',
            'genebe': '#2ca02c',
            'Genebe': '#2ca02c',
            'intervar': '#d62728',
            'Intervar': '#d62728',
            'tapes': '#9467bd',
            'TAPES': '#9467bd',
            'lirical': '#ff7f0e',
            'LIRICAL': '#ff7f0e',
            'cpsr': '#8c564b',
            'CPSR': '#8c564b'
        }
        for tool in tools:
            values = [
                self.metrics[tool]['ranking'].get('top_rank_percentage', 0),
                self.metrics[tool]['ranking'].get('top_5_percentage', 0),
                self.metrics[tool]['ranking'].get('top_10_percentage', 0),
                self.metrics[tool]['accuracy'].get('recall', 0) * 100
            ]
            values += values[:1]
            ax.plot(angles, values, linewidth=2, linestyle='solid', label=tool, color=tool_colors.get(tool, None))
            ax.fill(angles, values, alpha=0.1, color=tool_colors.get(tool, None))
        plt.legend(loc='center left', bbox_to_anchor=(1.3, 0.5))
        if 'cpsr' in tools:
            plt.title('Overall Tool Performance Metrics (Including CPSR)', size=16)
        else:
            plt.title('Overall Tool Performance Metrics (Excluding CPSR)', size=16)
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'radar_performance.png'), dpi=1200, bbox_inches='tight')
        plt.close()
        
        # 5. Top-N percentage comparison
        plt.figure(figsize=(14, 8))
        top_n_values = [1, 5, 10, 20, 30, 40, 50]
        for tool in tools:
            values = []
            for n in top_n_values:
                metric_name = f'top_rank_percentage' if n == 1 else f'top_{n}_percentage'
                values.append(self.metrics[tool]['ranking'].get(metric_name, 0))
            plt.plot(top_n_values, values, marker='o', linewidth=2, label=tool)
        plt.xlabel('Top-N', fontsize=12)
        plt.ylabel('Percentage of cases (%)', fontsize=12)
        plt.title('Top-N Percentage Comparison', fontsize=14)
        plt.xticks(top_n_values)
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'top_n_comparison.png'), dpi=1200, bbox_inches='tight')
        plt.close()
        
        # 6. Mean and median ranks
        plt.figure(figsize=(10, 6))
        mean_ranks = [self.metrics[tool]['ranking'].get('mean_rank', 0) for tool in tools]
        median_ranks = [self.metrics[tool]['ranking'].get('median_rank', 0) for tool in tools]
        x = np.arange(len(tools))
        width = 0.35
        plt.bar(x - width/2, mean_ranks, width, label='Mean Rank')
        plt.bar(x + width/2, median_ranks, width, label='Median Rank')
        plt.xlabel('Tool', fontsize=12)
        plt.ylabel('Rank', fontsize=12)
        plt.title('Mean and Median Ranks by Tool', fontsize=14)
        plt.xticks(x, tools, fontsize=12)
        plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), fontsize=14)
        plt.grid(True, axis='y', linestyle='--', alpha=0.7)
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'rank_comparison.png'), dpi=1200, bbox_inches='tight')
        plt.close()
        
        # 7. Precision at different thresholds
        plt.figure(figsize=(14, 8))
        thresholds = [1, 5, 10, 20, 50]
        for tool in tools:
            values = [self.metrics[tool]['accuracy'].get(f'precision_at_{threshold}', 0) for threshold in thresholds]
            plt.plot(thresholds, values, marker='o', linewidth=2, label=tool)
        plt.xlabel('Rank Threshold', fontsize=12)
        plt.ylabel('Precision', fontsize=12)
        plt.title('Precision at Different Rank Thresholds', fontsize=14)
        plt.xticks(thresholds)
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'precision_comparison.png'), dpi=1200, bbox_inches='tight')
        plt.close()
        
        # 8. F1 score and recall comparison
        plt.figure(figsize=(12, 6))
        f1_scores = [self.metrics[tool]['accuracy'].get('f1_score', 0) for tool in tools]
        recall_values = [self.metrics[tool]['accuracy'].get('recall', 0) for tool in tools]
        x = np.arange(len(tools))
        width = 0.35
        plt.bar(x - width/2, f1_scores, width, label='F1 Score')
        plt.bar(x + width/2, recall_values, width, label='Recall')
        plt.xlabel('Tool', fontsize=12)
        plt.ylabel('Score', fontsize=12)
        plt.title('F1 Score and Recall Comparison', fontsize=14)
        plt.xticks(x, tools)
        plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
        plt.grid(True, axis='y', linestyle='--', alpha=0.7)
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'f1_recall_comparison.png'), dpi=1200, bbox_inches='tight')
        plt.close()
        
        # 9. Generate CDF visualization
        plt.figure(figsize=(14, 8))
        for tool in tools:
            ranks = []
            for sample_id, true_gene in ground_truth.items():
                if sample_id in self.tool_data[tool]:
                    ranked_genes = self.tool_data[tool][sample_id]
                    if true_gene in ranked_genes:
                        rank = ranked_genes.index(true_gene) + 1
                    else:
                        rank = len(ranked_genes) + 1
                    ranks.append(rank)
            if ranks:
                x = np.sort(ranks)
                y = np.arange(1, len(x) + 1) / len(x)
                plt.step(x, y, label=tool, linewidth=2)
        plt.xlabel('Rank', fontsize=12)
        plt.ylabel('Cumulative Probability', fontsize=12)
        plt.title('Cumulative Distribution Function of Ranks', fontsize=14)
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'rank_cdf.png'), dpi=1200, bbox_inches='tight')
        plt.close()
        
        # 10. Venn diagram for tool overlap (for 2 or 3 tools)
        if len(tools) >= 2:
            plt.figure(figsize=(10, 8))
            tool_successes = {}
            for tool in tools:
                successful_samples = set()
                for sample_id, true_gene in ground_truth.items():
                    if sample_id in self.tool_data[tool]:
                        ranked_genes = self.tool_data[tool][sample_id]
                        top_n = min(10, len(ranked_genes))
                        if true_gene in ranked_genes[:top_n]:
                            successful_samples.add(sample_id)
                tool_successes[tool] = successful_samples
            if len(tools) == 2:
                venn2([tool_successes[tools[0]], tool_successes[tools[1]]], set_labels=(tools[0], tools[1]))
                plt.title('Overlap of Successful Identifications (Top-10)', fontsize=14)
            elif len(tools) == 3:
                venn3([tool_successes[tools[0]], tool_successes[tools[1]], tool_successes[tools[2]]], set_labels=(tools[0], tools[1], tools[2]))
                plt.title('Overlap of Successful Identifications (Top-10)', fontsize=14)
            plt.savefig(os.path.join(output_dir, 'tool_overlap_venn.png'), dpi=1200)
            plt.close()
        
        print(f"Visualizations saved to {output_dir} directory.")

    def update_report_with_statistics(self, f):
        """
        Update the report with statistical test results, including bootstrap CIs for top-1, top-5, top-10.
        
        Parameters:
        -----------
        f : file object
            The open file to write the report to
        """
        f.write("STATISTICAL TESTS\n")
        f.write("----------------\n")
    
        # Friedman Test
        if 'friedman_test' in self.statistical_tests:
            f.write("Friedman Test Results (Ranking Performance):\n")
            friedman = self.statistical_tests['friedman_test']
            f.write(f"  Statistic: {friedman.get('statistic', 'N/A'):.4f}\n")
            f.write(f"  p-value: {friedman.get('p_value', 'N/A'):.4f}\n")
            f.write(f"  Significant: {'Yes' if friedman.get('significant', False) else 'No'}\n\n")
    
        # Nemenyi Post-hoc Test
        if 'nemenyi_test' in self.statistical_tests and 'tool_pairs' in self.statistical_tests['nemenyi_test']:
            f.write("Nemenyi Post-hoc Test Results (Ranking Performance):\n")
            significant_pairs = [(t1, t2, data['p_value']) for (t1, t2), data in 
                                self.statistical_tests['nemenyi_test']['tool_pairs'].items() 
                                if data['significant']]
            if significant_pairs:
                f.write("  Significant tool pairs:\n")
                for t1, t2, p_val in sorted(significant_pairs, key=lambda x: x[2]):
                    tool1_acc = self.metrics[t1]['ranking'].get('top_rank_percentage', 0)
                    tool2_acc = self.metrics[t2]['ranking'].get('top_rank_percentage', 0)
                    better_tool = t1 if tool1_acc > tool2_acc else t2
                    f.write(f"    {t1} vs {t2}: p-value={p_val:.4f}, {better_tool} performs better\n")
            else:
                f.write("  No significant differences found between tool pairs.\n")
            f.write("\n")
    
        # Fisher's Exact Test
        if 'fisher_exact' in self.statistical_tests:
            f.write("Fisher's Exact Test Results:\n")
            for threshold, label in [('top_1', 'Top-1'), ('top_5', 'Top-5'), ('top_10', 'Top-10')]:
                if threshold in self.statistical_tests['fisher_exact']:
                    f.write(f"  {label} Comparison:\n")
                    significant_pairs = [(t1, t2, data['p_value'], data['odds_ratio']) 
                                        for (t1, t2), data in self.statistical_tests['fisher_exact'][threshold].items() 
                                        if data['significant']]
                    if significant_pairs:
                        f.write("    Significant tool pairs:\n")
                        for t1, t2, p_val, odds in sorted(significant_pairs, key=lambda x: x[2]):
                            better_tool = t1 if odds > 1 else t2
                            f.write(f"      {t1} vs {t2}: p-value={p_val:.4f}, {better_tool} performs better\n")
                    else:
                        f.write("    No significant differences found.\n")
            f.write("\n")
    
        # Bootstrap Confidence Intervals
        if 'bootstrap_ci' in self.statistical_tests:
            f.write("Bootstrap 95% Confidence Intervals:\n")
            f.write(f"{'Tool':<15}{'Top-1 (%)':<25}{'Top-5 (%)':<25}{'Top-10 (%)':<25}\n")
            f.write("-" * 100 + "\n")
            for tool in sorted(self.tool_data.keys()):
                if tool in self.statistical_tests['bootstrap_ci']:
                    ci_data = self.statistical_tests['bootstrap_ci'][tool]
                    f.write(f"{tool:<15}")
                    for metric in ['top_rank_percentage', 'top_5_percentage', 'top_10_percentage']:
                        if metric in ci_data:
                            data = ci_data[metric]
                            f.write(f"{data['mean']:.2f} ({data['lower']:.2f}-{data['upper']:.2f}){' '*(25-13)}")
                        else:
                            f.write(f"N/A{' '*(25-13)}")
                    f.write("\n")
            f.write("\n")

    def generate_report(self, output_file='benchmark_report.txt'):
        """
        Generate a comprehensive report of the results.
        
        Parameters:
        -----------
        output_file : str
            Path to save the report
        """
        print("Generating report...")
        
        with open(output_file, 'w') as f:
            f.write("========================================================\n")
            f.write("   VARIANT CLASSIFICATION TOOLS BENCHMARKING REPORT     \n")
            f.write("========================================================\n\n")
            
            # Summary of tools analyzed
            f.write("TOOLS ANALYZED\n")
            f.write("-------------\n")
            for tool in self.tool_data.keys():
                f.write(f"- {tool}\n")
            f.write("\n")
            
            # Ranking Metrics
            f.write("RANKING METRICS\n")
            f.write("--------------\n")
            metrics = ['Top-Rank %', 'Top-5 %', 'Top-10 %', 'Top-20 %', 'Mean Rank', 'Median Rank']
            f.write(f"{'Tool':<15}")
            for metric in metrics:
                f.write(f"{metric:<15}")
            f.write("\n")
            f.write("-" * (15 * (len(metrics) + 1)) + "\n")
            for tool in self.tool_data.keys():
                f.write(f"{tool:<15}")
                value = self.metrics[tool]['ranking'].get('top_rank_percentage', np.nan)
                f.write(f"{value:,.2f}%{' ' * 8}")
                value = self.metrics[tool]['ranking'].get('top_5_percentage', np.nan)
                f.write(f"{value:,.2f}%{' ' * 8}")
                value = self.metrics[tool]['ranking'].get('top_10_percentage', np.nan)
                f.write(f"{value:,.2f}%{' ' * 8}")
                value = self.metrics[tool]['ranking'].get('top_20_percentage', np.nan)
                f.write(f"{value:,.2f}%{' ' * 8}")
                value = self.metrics[tool]['ranking'].get('mean_rank', np.nan)
                f.write(f"{value:,.2f}{' ' * 10}")
                value = self.metrics[tool]['ranking'].get('median_rank', np.nan)
                f.write(f"{value:,.2f}{' ' * 10}")
                f.write("\n")
            f.write("\n")
            
            # Filtering Performance
            f.write("FILTERING PERFORMANCE\n")
            f.write("--------------------\n")
            metrics = ['Filtered-Out Rate', 'Retention Rate']
            f.write(f"{'Tool':<15}")
            for metric in metrics:
                f.write(f"{metric:<20}")
            f.write("\n")
            f.write("-" * (15 + 20 * len(metrics)) + "\n")
            for tool in self.tool_data.keys():
                f.write(f"{tool:<15}")
                value = self.metrics[tool]['filtering'].get('filtered_out_rate', np.nan)
                f.write(f"{value:,.2f}%{' ' * 13}")
                value = self.metrics[tool]['filtering'].get('retention_rate', np.nan)
                f.write(f"{value:,.2f}%{' ' * 13}")
                f.write("\n")
            f.write("\n")
            
            # Accuracy Metrics
            f.write("ACCURACY METRICS\n")
            f.write("---------------\n")
            metrics = ['Precision@1', 'Precision@10', 'Recall', 'F1 Score']
            f.write(f"{'Tool':<15}")
            for metric in metrics:
                f.write(f"{metric:<15}")
            f.write("\n")
            f.write("-" * (15 * (len(metrics) + 1)) + "\n")
            for tool in self.tool_data.keys():
                f.write(f"{tool:<15}")
                value = self.metrics[tool]['accuracy'].get('precision_at_1', np.nan)
                f.write(f"{value:,.4f}{' ' * 8}")
                value = self.metrics[tool]['accuracy'].get('precision_at_10', np.nan)
                f.write(f"{value:,.4f}{' ' * 8}")
                value = self.metrics[tool]['accuracy'].get('recall', np.nan)
                f.write(f"{value:,.4f}{' ' * 8}")
                value = self.metrics[tool]['accuracy'].get('f1_score', np.nan)
                f.write(f"{value:,.4f}{' ' * 8}")
                f.write("\n")
            f.write("\n")
            
            # Add statistical tests
            self.update_report_with_statistics(f)
            
            # Conclusion
            f.write("CONCLUSION\n")
            f.write("----------\n")
            f.write("Based on the benchmarking metrics, the tools can be ranked as follows:\n\n")
            tool_ranking = {tool: self.metrics[tool]['ranking'].get('top_10_percentage', 0) for tool in self.tool_data.keys()}
            sorted_tools = sorted(tool_ranking.items(), key=lambda x: x[1], reverse=True)
            for i, (tool, value) in enumerate(sorted_tools):
                f.write(f"{i+1}. {tool} (Top-10: {value:.2f}%)\n")
            f.write("\nRecommendations:\n")
            if 'cpsr' in self.tool_data and 'LIRICAL' in self.tool_data:
                f.write("1. For cancer samples, CPSR should be the first-line tool, with LIRICAL as complement\n")
                f.write("2. For non-cancer samples, LIRICAL should be the primary tool\n")
            elif 'LIRICAL' in self.tool_data:
                f.write("1. LIRICAL shows the strongest overall performance across metrics\n")
            f.write("3. Consider using multiple tools in combination to maximize variant identification\n")
            f.write("4. For specific applications, consider the strengths of each tool highlighted in this report\n")
            f.write("5. Franklin performs well at finding pathogenic variants but may need alternative ranking metrics\n")
        
        print(f"Report generated and saved to {output_file}")

def main():
    """Main function to run the benchmark analysis."""
    base_dir = r"C:\Users\z5537966\OneDrive - UNSW\Desktop\new data\test\common_samples_only\Franklin\manual\final_combined_data"
    unified_acmg_path = os.path.join(base_dir, "ACMG_163.csv")
    lirical_folder_path = base_dir
    manual_annotations_path = os.path.join(base_dir, "hgnc_standardized_matched_manual_annotations _163.xlsx")
    
    viz_output_dir = os.path.join(base_dir, "benchmark_visualizations")
    report_output_file = os.path.join(base_dir, "benchmark_report.txt")
    cancer_viz_output_dir = os.path.join(base_dir, "cancer_benchmark_visualizations")
    cancer_report_output_file = os.path.join(base_dir, "cancer_benchmark_report.txt")
    
    print("\n===== BENCHMARKING ALL SAMPLES (EXCLUDING CPSR AND CHARGER) =====")
    benchmarker = VariantToolBenchmarker(
        unified_acmg_path,
        lirical_folder_path,
        manual_annotations_path,
        exclude_tools=['charger', 'CPSR']
    )
    benchmarker.load_data()
    benchmarker.calculate_metrics()
    benchmarker.perform_and_visualize_statistical_tests(output_dir=viz_output_dir)
    benchmarker.plot_friedman_ranks(output_dir=viz_output_dir)
    benchmarker.generate_visualizations(output_dir=viz_output_dir)
    benchmarker.generate_report(output_file=report_output_file)
    print(f"\nResults saved to:\nVisualizations: {viz_output_dir}\nReport: {report_output_file}")
    
    print("\n\n===== BENCHMARKING CANCER SAMPLES (INCLUDING CPSR) =====")
    cancer_samples = [
        'PGERA2440', 'PGERA1112', 'PGERA2125', 'PGERA1788', 'G5500', 'G5620', 
        'G2001380', 'G2101361', 'G1800091', 'G1800228', 
        'G2200657', 'G1900091', 'G2000091', 'G1900228', 'G2000228', 'G2101380', 
        'G2201380', 'G2201360', 'G2301360', 'G2300657', 'G2400657', 'G6500', 
        'G7500', 'G7620', 'PGERA2112', 'PGERA3112', 'PGERA2788', 
        'PGERA3788', 'PGERA3125', 'PGERA4125', 'PGERA3440', 'PGERA4440'
    ]
    cancer_benchmarker = VariantToolBenchmarker(
        unified_acmg_path,
        lirical_folder_path,
        manual_annotations_path,
        exclude_tools=['charger'], 
        sample_subset=cancer_samples
    )
    cancer_benchmarker.load_data()
    cancer_benchmarker.calculate_metrics()
    cancer_benchmarker.perform_and_visualize_statistical_tests(output_dir=cancer_viz_output_dir)
    cancer_benchmarker.plot_friedman_ranks(output_dir=cancer_viz_output_dir)
    cancer_benchmarker.generate_visualizations(output_dir=cancer_viz_output_dir)
    cancer_benchmarker.generate_report(output_file=cancer_report_output_file)
    print(f"\nCancer-specific results saved to:\nVisualizations: {cancer_viz_output_dir}\nReport: {cancer_report_output_file}")

if __name__ == "__main__":
    main()
